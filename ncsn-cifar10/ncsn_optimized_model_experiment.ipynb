{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fvc9WHmhm00",
        "outputId": "07029780-6b38-4174-9d16-98e5efcc8bc9"
      },
      "outputs": [],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFleBl4MhrIF"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBnuYBCfhw3W",
        "outputId": "052f7ad7-c425-4e07-cf8c-1c7b2b4734e0"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "XqjrzYJbhxpS",
        "outputId": "9f1f54f8-f852-4195-b9c5-0191956c47d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Key Improvements over Baseline NCSN:\n",
        "# 1. Improved noise conditioning (score = network_output / sigma)\n",
        "# 2. Self-attention mechanisms at multiple resolutions\n",
        "# 3. Exponential Moving Average (EMA) of model weights\n",
        "# 4. Better noise scale selection\n",
        "# 5. Spectral normalization for stability\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "# ==================== Improved Model Architecture ====================\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.query = spectral_norm(nn.Conv2d(in_channels, in_channels // 8, 1))\n",
        "        self.key = spectral_norm(nn.Conv2d(in_channels, in_channels // 8, 1))\n",
        "        self.value = spectral_norm(nn.Conv2d(in_channels, in_channels, 1))\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "\n",
        "        query = self.query(x).view(batch_size, -1, H * W)\n",
        "        key = self.key(x).view(batch_size, -1, H * W)\n",
        "        value = self.value(x).view(batch_size, -1, H * W)\n",
        "\n",
        "        attention = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, H, W)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImprovedConditionalInstanceNorm2dPlus(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, bias=True):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.bias = bias\n",
        "        self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n",
        "\n",
        "        if bias:\n",
        "            self.embed = nn.Embedding(num_classes, num_features * 3)\n",
        "            self.embed.weight.data[:, :2 * num_features].normal_(1, 0.02)\n",
        "            self.embed.weight.data[:, 2 * num_features:].zero_()\n",
        "        else:\n",
        "            self.embed = nn.Embedding(num_classes, 2 * num_features)\n",
        "            self.embed.weight.data.normal_(1, 0.02)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.instance_norm(x)\n",
        "        if self.bias:\n",
        "            gamma, alpha, beta = self.embed(y).chunk(3, dim=-1)\n",
        "            out = gamma.view(-1, self.num_features, 1, 1) * h + alpha.view(-1, self.num_features, 1, 1) * x + beta.view(-1, self.num_features, 1, 1)\n",
        "        else:\n",
        "            gamma, alpha = self.embed(y).chunk(2, dim=-1)\n",
        "            out = gamma.view(-1, self.num_features, 1, 1) * h + alpha.view(-1, self.num_features, 1, 1) * x\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImprovedConditionalResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_classes, resample=None, activation=nn.ELU(), use_attention=False):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.resample = resample\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        self.norm1 = ImprovedConditionalInstanceNorm2dPlus(in_channels, num_classes)\n",
        "        self.norm2 = ImprovedConditionalInstanceNorm2dPlus(out_channels, num_classes)\n",
        "\n",
        "        if resample == 'down':\n",
        "            self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, 3, padding=1))\n",
        "            self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, 3, padding=1, stride=2))\n",
        "            self.shortcut = spectral_norm(nn.Conv2d(in_channels, out_channels, 1, stride=2))\n",
        "        elif resample == 'up':\n",
        "            self.conv1 = spectral_norm(nn.ConvTranspose2d(in_channels, out_channels, 3, padding=1))\n",
        "            self.conv2 = spectral_norm(nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1, stride=2, output_padding=1))\n",
        "            self.shortcut = spectral_norm(nn.ConvTranspose2d(in_channels, out_channels, 1, stride=2, output_padding=1))\n",
        "        else:\n",
        "            self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, 3, padding=1))\n",
        "            self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n",
        "            self.shortcut = spectral_norm(nn.Conv2d(in_channels, out_channels, 1)) if in_channels != out_channels else None\n",
        "\n",
        "        # Self-attention\n",
        "        if use_attention:\n",
        "            self.attention = SelfAttention(out_channels)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.norm1(x, y)\n",
        "        h = self.activation(h)\n",
        "        h = self.conv1(h)\n",
        "        h = self.norm2(h, y)\n",
        "        h = self.activation(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.shortcut is not None:\n",
        "            out = h + self.shortcut(x)\n",
        "        else:\n",
        "            out = h + x\n",
        "\n",
        "        # Apply attention\n",
        "        if self.use_attention:\n",
        "            out = self.attention(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImprovedRefineNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_classes, use_attention=False):\n",
        "        super().__init__()\n",
        "        self.refine1 = ImprovedConditionalResidualBlock(in_channels, out_channels, num_classes, resample=None, use_attention=False)\n",
        "        self.refine2 = ImprovedConditionalResidualBlock(out_channels, out_channels, num_classes, resample=None, use_attention=use_attention)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.refine1(x, y)\n",
        "        h = self.refine2(h, y)\n",
        "        return h\n",
        "\n",
        "\n",
        "class ImprovedNCSNModel(nn.Module):\n",
        "    def __init__(self, num_classes=10, ngf=128, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.ngf = ngf\n",
        "        self.use_attention = use_attention\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "        self.begin_conv = spectral_norm(nn.Conv2d(3, ngf, 3, padding=1))\n",
        "\n",
        "        self.res1 = ImprovedConditionalResidualBlock(ngf, ngf, num_classes, resample='down', use_attention=False)\n",
        "        self.res2 = ImprovedConditionalResidualBlock(ngf, 2*ngf, num_classes, resample='down', use_attention=use_attention)  # Attention at 8x8\n",
        "        self.res3 = ImprovedConditionalResidualBlock(2*ngf, 2*ngf, num_classes, resample='down', use_attention=False)\n",
        "\n",
        "        self.res4 = ImprovedConditionalResidualBlock(2*ngf, 2*ngf, num_classes, resample=None, use_attention=False)\n",
        "\n",
        "        self.refine1 = ImprovedRefineNet(2*ngf, 2*ngf, num_classes, use_attention=False)\n",
        "        self.res5 = ImprovedConditionalResidualBlock(2*ngf, 2*ngf, num_classes, resample='up', use_attention=False)\n",
        "\n",
        "        self.refine2 = ImprovedRefineNet(2*ngf, 2*ngf, num_classes, use_attention=use_attention)  # Attention at 8x8\n",
        "        self.res6 = ImprovedConditionalResidualBlock(2*ngf, ngf, num_classes, resample='up', use_attention=False)\n",
        "\n",
        "        self.refine3 = ImprovedRefineNet(ngf, ngf, num_classes, use_attention=False)\n",
        "        self.res7 = ImprovedConditionalResidualBlock(ngf, ngf, num_classes, resample='up', use_attention=False)\n",
        "\n",
        "        self.refine4 = ImprovedRefineNet(ngf, ngf, num_classes, use_attention=False)\n",
        "\n",
        "        self.norm_final = ImprovedConditionalInstanceNorm2dPlus(ngf, num_classes)\n",
        "        self.end_conv = spectral_norm(nn.Conv2d(ngf, 3, 3, padding=1))\n",
        "\n",
        "    def forward(self, x, y, sigmas=None):\n",
        "        h = self.begin_conv(x)\n",
        "\n",
        "        h1 = self.res1(h, y)\n",
        "        h2 = self.res2(h1, y)\n",
        "        h3 = self.res3(h2, y)\n",
        "\n",
        "        h = self.res4(h3, y)\n",
        "\n",
        "        h = self.refine1(h, y)\n",
        "        h = self.res5(h, y)\n",
        "        h = h + h2\n",
        "\n",
        "        h = self.refine2(h, y)\n",
        "        h = self.res6(h, y)\n",
        "        h = h + h1\n",
        "\n",
        "        h = self.refine3(h, y)\n",
        "        h = self.res7(h, y)\n",
        "\n",
        "        h = self.refine4(h, y)\n",
        "\n",
        "        h = self.norm_final(h, y)\n",
        "        h = self.activation(h)\n",
        "        h = self.end_conv(h)\n",
        "\n",
        "        if sigmas is not None:\n",
        "            used_sigmas = sigmas[y].view(-1, 1, 1, 1)\n",
        "            h = h / used_sigmas\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "# ==================== Exponential Moving Average ====================\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "        self.register()\n",
        "\n",
        "    def register(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "                self.shadow[name] = new_average.clone()\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                self.backup[name] = param.data\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.backup\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "\n",
        "\n",
        "# ==================== Loss Functions ====================\n",
        "def improved_anneal_dsm_score_estimation(scorenet, samples, sigmas, anneal_power=2.0):\n",
        "    batch_size = samples.shape[0]\n",
        "\n",
        "    labels = torch.randint(0, len(sigmas), (batch_size,), device=samples.device)\n",
        "    used_sigmas = sigmas[labels].view(batch_size, *([1] * len(samples.shape[1:])))\n",
        "\n",
        "    noise = torch.randn_like(samples) * used_sigmas\n",
        "    perturbed_samples = samples + noise\n",
        "\n",
        "    # Forward pass with sigmas\n",
        "    predicted_score = scorenet(perturbed_samples, labels, sigmas)\n",
        "    target = -noise / (used_sigmas ** 2)\n",
        "\n",
        "    losses = 0.5 * ((predicted_score - target) ** 2).sum(dim=(1, 2, 3))\n",
        "    loss_weights = (used_sigmas.squeeze() ** anneal_power)\n",
        "    weighted_loss = (losses * loss_weights).mean()\n",
        "\n",
        "\n",
        "\n",
        "    unweighted_loss = losses.mean()\n",
        "    channel_losses = 0.5 * ((predicted_score - target) ** 2).sum(dim=(2, 3))\n",
        "\n",
        "    loss_per_channel = {\n",
        "        'loss_channel_0': channel_losses[:, 0].mean().item(),\n",
        "        'loss_channel_1': channel_losses[:, 1].mean().item(),\n",
        "        'loss_channel_2': channel_losses[:, 2].mean().item(),\n",
        "    }\n",
        "\n",
        "    unique_labels = torch.unique(labels)\n",
        "    loss_per_sigma = {}\n",
        "    for lbl in unique_labels:\n",
        "        mask = labels == lbl\n",
        "        if mask.sum() > 0:\n",
        "            loss_per_sigma[f'loss_sigma_{lbl.item()}'] = losses[mask].mean().item()\n",
        "\n",
        "    l1_loss = torch.abs(predicted_score - target).sum(dim=(1, 2, 3)).mean()\n",
        "    grad_norm = torch.norm(predicted_score.view(batch_size, -1), dim=1).mean()\n",
        "    target_norm = torch.norm(target.view(batch_size, -1), dim=1).mean()\n",
        "\n",
        "    loss_dict = {\n",
        "        'loss_weighted': weighted_loss.item(),\n",
        "        'loss_unweighted': unweighted_loss.item(),\n",
        "        'loss_l1': l1_loss.item(),\n",
        "        'grad_norm': grad_norm.item(),\n",
        "        'target_norm': target_norm.item(),\n",
        "        'sigma_mean': used_sigmas.mean().item(),\n",
        "        'sigma_std': used_sigmas.std().item(),\n",
        "        **loss_per_channel,\n",
        "        **loss_per_sigma\n",
        "    }\n",
        "\n",
        "    return weighted_loss, loss_dict\n",
        "\n",
        "\n",
        "# ==================== Sampling ====================\n",
        "@torch.no_grad()\n",
        "def improved_anneal_langevin_dynamics(scorenet, x_init, sigmas, n_steps_each=100, step_lr=0.00002,\n",
        "                                      log_intermediate=False, log_interval=10):\n",
        "    x = x_init.clone()\n",
        "    intermediate_images = []\n",
        "\n",
        "    for idx, sigma in enumerate(sigmas):\n",
        "        sigma_val = sigma.item()\n",
        "        labels = torch.ones(x.shape[0], device=x.device, dtype=torch.long) * idx\n",
        "        step_size = step_lr * (sigma_val / sigmas[-1].item()) ** 2\n",
        "\n",
        "        for step in range(n_steps_each):\n",
        "            noise = torch.randn_like(x) * np.sqrt(step_size * 2)\n",
        "            grad = scorenet(x, labels, sigmas)\n",
        "            x = x + step_size * grad + noise\n",
        "\n",
        "            if log_intermediate and (step % log_interval == 0 or step == n_steps_each - 1):\n",
        "                intermediate_images.append({\n",
        "                    'sigma_idx': idx,\n",
        "                    'step': step,\n",
        "                    'image': x.clone()\n",
        "                })\n",
        "\n",
        "    if log_intermediate:\n",
        "        return x, intermediate_images\n",
        "    return x\n",
        "\n",
        "\n",
        "# ==================== Configuration ====================\n",
        "class ImprovedNCSNConfig:\n",
        "    num_classes = 10\n",
        "    ngf = 128\n",
        "    use_attention = True\n",
        "\n",
        "    sigma_begin = 50.0  # Larger initial noise\n",
        "    sigma_end = 0.01\n",
        "\n",
        "    batch_size = 128\n",
        "    num_epochs = 100\n",
        "    lr = 0.0001  # Lower learning rate\n",
        "    lr_decay_factor = 0.1\n",
        "    lr_decay_epochs = [70, 90]\n",
        "\n",
        "    # EMA\n",
        "    ema_decay = 0.999\n",
        "    use_ema = True\n",
        "\n",
        "    n_steps_each = 100\n",
        "    step_lr = 0.00002\n",
        "\n",
        "    image_size = 32\n",
        "    num_workers = 4\n",
        "    prefetch_factor = 2\n",
        "\n",
        "    log_interval = 50\n",
        "    sample_interval = 500\n",
        "    num_samples = 64\n",
        "\n",
        "    checkpoint_dir = '/content/drive/MyDrive/cs236/assignments/final/checkpoints_improved/'\n",
        "    save_every_n_epochs = 10\n",
        "    resume_from_checkpoint = None\n",
        "\n",
        "\n",
        "# ==================== Data Loading ====================\n",
        "def get_cifar10_dataloaders(batch_size, num_workers=4, prefetch_factor=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "        prefetch_factor=prefetch_factor,\n",
        "        persistent_workers=True if num_workers > 0 else False\n",
        "    )\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "# ==================== Visualization ====================\n",
        "def denormalize(x):\n",
        "    return (x + 1) / 2\n",
        "\n",
        "\n",
        "def save_sample_images(samples, filename, nrow=8):\n",
        "    samples = denormalize(samples)\n",
        "    samples = torch.clamp(samples, 0, 1)\n",
        "    grid = torchvision.utils.make_grid(samples, nrow=nrow, padding=2)\n",
        "\n",
        "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, bbox_inches='tight', dpi=150)\n",
        "    plt.close()\n",
        "    return grid\n",
        "\n",
        "\n",
        "def create_intermediate_grid(intermediate_images, num_samples=8):\n",
        "    if not intermediate_images:\n",
        "        return None\n",
        "\n",
        "    selected_imgs = []\n",
        "    for img_data in intermediate_images[::max(1, len(intermediate_images)//10)]:\n",
        "        imgs = img_data['image'][:num_samples]\n",
        "        selected_imgs.append(denormalize(torch.clamp(imgs, -1, 1)))\n",
        "\n",
        "    if selected_imgs:\n",
        "        all_imgs = torch.cat(selected_imgs, dim=0)\n",
        "        grid = torchvision.utils.make_grid(all_imgs, nrow=num_samples, padding=2)\n",
        "        return grid\n",
        "    return None\n",
        "\n",
        "\n",
        "# ==================== Checkpoint Management ====================\n",
        "def save_checkpoint(model, optimizer, scheduler, ema, epoch, loss, sigmas, filepath):\n",
        "    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'ema_shadow': ema.shadow if ema else None,\n",
        "        'loss': loss,\n",
        "        'sigmas': sigmas.cpu() if torch.is_tensor(sigmas) else sigmas,\n",
        "        'config': {\n",
        "            'num_classes': ImprovedNCSNConfig.num_classes,\n",
        "            'ngf': ImprovedNCSNConfig.ngf,\n",
        "            'sigma_begin': ImprovedNCSNConfig.sigma_begin,\n",
        "            'sigma_end': ImprovedNCSNConfig.sigma_end,\n",
        "            'use_attention': ImprovedNCSNConfig.use_attention,\n",
        "        }\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath} (epoch {epoch})\")\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, model, optimizer=None, scheduler=None, ema=None, device='cpu'):\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Checkpoint not found: {filepath}\")\n",
        "        return 0\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    if ema is not None and 'ema_shadow' in checkpoint and checkpoint['ema_shadow']:\n",
        "        ema.shadow = checkpoint['ema_shadow']\n",
        "\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    print(f\"Checkpoint loaded from epoch {epoch}\")\n",
        "    return epoch + 1\n",
        "\n",
        "\n",
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoint_dir = Path(checkpoint_dir)\n",
        "\n",
        "    if not checkpoint_dir.exists():\n",
        "        return None\n",
        "\n",
        "    latest_files = list(checkpoint_dir.glob('improved_ncsn_latest_*.pth'))\n",
        "\n",
        "    if latest_files:\n",
        "        latest_checkpoint = None\n",
        "        latest_epoch = -1\n",
        "\n",
        "        for ckpt_path in latest_files:\n",
        "            try:\n",
        "                epoch_str = ckpt_path.stem.split('_')[-1]\n",
        "                epoch_num = int(epoch_str)\n",
        "\n",
        "                if epoch_num > latest_epoch:\n",
        "                    latest_epoch = epoch_num\n",
        "                    latest_checkpoint = ckpt_path\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "        if latest_checkpoint:\n",
        "            print(f\"Found latest checkpoint: {latest_checkpoint} (Epoch {latest_epoch})\")\n",
        "            return str(latest_checkpoint)\n",
        "\n",
        "    checkpoint_files = list(checkpoint_dir.glob('improved_ncsn_epoch_*.pth'))\n",
        "\n",
        "    if not checkpoint_files:\n",
        "        return None\n",
        "\n",
        "    latest_checkpoint = None\n",
        "    latest_epoch = -1\n",
        "\n",
        "    for ckpt_path in checkpoint_files:\n",
        "        try:\n",
        "            epoch_str = ckpt_path.stem.split('_')[-1]\n",
        "            epoch_num = int(epoch_str)\n",
        "\n",
        "            if epoch_num > latest_epoch:\n",
        "                latest_epoch = epoch_num\n",
        "                latest_checkpoint = ckpt_path\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "    if latest_checkpoint:\n",
        "        print(f\"Found latest checkpoint: {latest_checkpoint} (Epoch {latest_epoch})\")\n",
        "        return str(latest_checkpoint)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# ==================== Train ====================\n",
        "def train_improved_ncsn():\n",
        "    wandb.init(\n",
        "        entity=\"tourists\",\n",
        "        project=\"ncsn-cifar10\",\n",
        "        name=\"improved-ncsn\",\n",
        "        config={\n",
        "            \"model\": \"Improved NCSN (NCSNv2)\",\n",
        "            \"dataset\": \"CIFAR-10\",\n",
        "            \"improvements\": \"attention+ema+spectral_norm+improved_conditioning\",\n",
        "            \"num_classes\": ImprovedNCSNConfig.num_classes,\n",
        "            \"ngf\": ImprovedNCSNConfig.ngf,\n",
        "            \"use_attention\": ImprovedNCSNConfig.use_attention,\n",
        "            \"batch_size\": ImprovedNCSNConfig.batch_size,\n",
        "            \"num_epochs\": ImprovedNCSNConfig.num_epochs,\n",
        "            \"lr\": ImprovedNCSNConfig.lr,\n",
        "            \"ema_decay\": ImprovedNCSNConfig.ema_decay,\n",
        "            \"sigma_begin\": ImprovedNCSNConfig.sigma_begin,\n",
        "            \"sigma_end\": ImprovedNCSNConfig.sigma_end,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Improved noise schedule\n",
        "    sigmas = torch.tensor(\n",
        "        np.exp(np.linspace(\n",
        "            np.log(ImprovedNCSNConfig.sigma_begin),\n",
        "            np.log(ImprovedNCSNConfig.sigma_end),\n",
        "            ImprovedNCSNConfig.num_classes\n",
        "        ))\n",
        "    ).float().to(device)\n",
        "    print(f\"Improved noise levels (sigmas): {sigmas}\")\n",
        "\n",
        "    # Improved model\n",
        "    model = ImprovedNCSNModel(\n",
        "        num_classes=ImprovedNCSNConfig.num_classes,\n",
        "        ngf=ImprovedNCSNConfig.ngf,\n",
        "        use_attention=ImprovedNCSNConfig.use_attention\n",
        "    ).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # EMA\n",
        "    ema = EMA(model, decay=ImprovedNCSNConfig.ema_decay) if ImprovedNCSNConfig.use_ema else None\n",
        "    if ema:\n",
        "        print(f\"Using EMA with decay={ImprovedNCSNConfig.ema_decay}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=ImprovedNCSNConfig.lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=ImprovedNCSNConfig.lr_decay_epochs,\n",
        "        gamma=ImprovedNCSNConfig.lr_decay_factor\n",
        "    )\n",
        "\n",
        "    start_epoch = 0\n",
        "    if ImprovedNCSNConfig.resume_from_checkpoint:\n",
        "        start_epoch = load_checkpoint(\n",
        "            ImprovedNCSNConfig.resume_from_checkpoint, model, optimizer, scheduler, ema, device\n",
        "        )\n",
        "    else:\n",
        "        latest_ckpt = find_latest_checkpoint(ImprovedNCSNConfig.checkpoint_dir)\n",
        "        if latest_ckpt:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"RESUMING TRAINING from {latest_ckpt}\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "            start_epoch = load_checkpoint(latest_ckpt, model, optimizer, scheduler, ema, device)\n",
        "        else:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"STARTING FRESH TRAINING (Improved NCSNv2)\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "\n",
        "    train_loader = get_cifar10_dataloaders(\n",
        "        ImprovedNCSNConfig.batch_size,\n",
        "        ImprovedNCSNConfig.num_workers,\n",
        "        ImprovedNCSNConfig.prefetch_factor\n",
        "    )\n",
        "\n",
        "    # ==================== Training loop ====================\n",
        "    global_step = start_epoch * len(train_loader)\n",
        "\n",
        "    for epoch in range(start_epoch, ImprovedNCSNConfig.num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_metrics = {}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{ImprovedNCSNConfig.num_epochs}\")\n",
        "\n",
        "        for batch_idx, (images, _) in enumerate(pbar):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss, loss_dict = improved_anneal_dsm_score_estimation(model, images, sigmas)\n",
        "            loss.backward()\n",
        "\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            loss_dict['grad_norm_clipped'] = grad_norm.item()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update EMA\n",
        "            if ema:\n",
        "                ema.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "\n",
        "            for key, val in loss_dict.items():\n",
        "                if key not in epoch_metrics:\n",
        "                    epoch_metrics[key] = []\n",
        "                epoch_metrics[key].append(val)\n",
        "\n",
        "            if global_step % ImprovedNCSNConfig.log_interval == 0:\n",
        "                log_dict = {\n",
        "                    \"train/loss\": loss.item(),\n",
        "                    \"train/epoch\": epoch,\n",
        "                    \"train/lr\": optimizer.param_groups[0]['lr'],\n",
        "                    \"train/global_step\": global_step,\n",
        "                }\n",
        "                for key, val in loss_dict.items():\n",
        "                    log_dict[f\"train/{key}\"] = val\n",
        "\n",
        "                wandb.log(log_dict, step=global_step)\n",
        "\n",
        "            # Generate samples using EMA model\n",
        "            if global_step % ImprovedNCSNConfig.sample_interval == 0:\n",
        "                # Use EMA weights for sampling\n",
        "                if ema:\n",
        "                    ema.apply_shadow()\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    x_init = torch.randn(ImprovedNCSNConfig.num_samples, 3, 32, 32).to(device) * sigmas[0]\n",
        "\n",
        "                    samples, intermediate_imgs = improved_anneal_langevin_dynamics(\n",
        "                        model, x_init, sigmas,\n",
        "                        n_steps_each=ImprovedNCSNConfig.n_steps_each,\n",
        "                        step_lr=ImprovedNCSNConfig.step_lr,\n",
        "                        log_intermediate=True,\n",
        "                        log_interval=20\n",
        "                    )\n",
        "\n",
        "                    sample_path = f'{ImprovedNCSNConfig.checkpoint_dir}samples/improved_ncsn_samples_step_{global_step}.png'\n",
        "                    grid = save_sample_images(samples, sample_path)\n",
        "\n",
        "                    prog_grid = create_intermediate_grid(intermediate_imgs, num_samples=8)\n",
        "\n",
        "                    if prog_grid is not None:\n",
        "                        prog_path = f'{ImprovedNCSNConfig.checkpoint_dir}samples/improved_ncsn_progression_step_{global_step}.png'\n",
        "                        Path(prog_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "                        plt.figure(figsize=(16, 10))\n",
        "                        plt.imshow(prog_grid.permute(1, 2, 0).cpu().numpy())\n",
        "                        plt.axis('off')\n",
        "                        plt.title(f'Improved NCSN Sampling Progression - Step {global_step}')\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(prog_path, bbox_inches='tight', dpi=150)\n",
        "                        plt.close()\n",
        "\n",
        "                    wandb_logs = {\n",
        "                        \"samples/final\": wandb.Image(\n",
        "                            grid.permute(1, 2, 0).cpu().numpy(),\n",
        "                            caption=f\"Improved NCSN samples - Step {global_step}\"\n",
        "                        )\n",
        "                    }\n",
        "\n",
        "                    if prog_grid is not None:\n",
        "                        wandb_logs[\"samples/progression\"] = wandb.Image(\n",
        "                            prog_grid.permute(1, 2, 0).cpu().numpy(),\n",
        "                            caption=f\"Sampling progression - Step {global_step}\"\n",
        "                        )\n",
        "\n",
        "                    sample_images = denormalize(torch.clamp(samples[:16], -1, 1))\n",
        "                    for idx in range(min(16, sample_images.shape[0])):\n",
        "                        wandb_logs[f\"samples/individual_{idx}\"] = wandb.Image(\n",
        "                            sample_images[idx].permute(1, 2, 0).cpu().numpy(),\n",
        "                            caption=f\"Sample {idx} - Step {global_step}\"\n",
        "                        )\n",
        "\n",
        "                    wandb.log(wandb_logs, step=global_step)\n",
        "                    print(f\"\\n✓ Generated and logged {ImprovedNCSNConfig.num_samples} samples at step {global_step}\")\n",
        "\n",
        "                # Restore original weights\n",
        "                if ema:\n",
        "                    ema.restore()\n",
        "\n",
        "                model.train()\n",
        "\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1} - Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        epoch_avg_metrics = {\n",
        "            f\"epoch/{key}\": np.mean(vals)\n",
        "            for key, vals in epoch_metrics.items()\n",
        "        }\n",
        "        epoch_avg_metrics[\"epoch/avg_loss\"] = avg_epoch_loss\n",
        "        epoch_avg_metrics[\"epoch/number\"] = epoch + 1\n",
        "\n",
        "        wandb.log(epoch_avg_metrics, step=global_step)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % ImprovedNCSNConfig.save_every_n_epochs == 0:\n",
        "            checkpoint_path = f'{ImprovedNCSNConfig.checkpoint_dir}improved_ncsn_epoch_{epoch+1:03d}.pth'\n",
        "            save_checkpoint(model, optimizer, scheduler, ema, epoch, avg_epoch_loss, sigmas, checkpoint_path)\n",
        "\n",
        "        latest_path = f'{ImprovedNCSNConfig.checkpoint_dir}improved_ncsn_latest_{epoch+1:03d}.pth'\n",
        "\n",
        "        old_latest_files = list(Path(ImprovedNCSNConfig.checkpoint_dir).glob('improved_ncsn_latest_*.pth'))\n",
        "        for old_file in old_latest_files:\n",
        "            try:\n",
        "                old_file.unlink()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        save_checkpoint(model, optimizer, scheduler, ema, epoch, avg_epoch_loss, sigmas, latest_path)\n",
        "\n",
        "    # Final model save with EMA\n",
        "    if ema:\n",
        "        ema.apply_shadow()\n",
        "\n",
        "    final_path = f'{ImprovedNCSNConfig.checkpoint_dir}improved_ncsn_final.pth'\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'sigmas': sigmas.cpu(),\n",
        "        'config': {\n",
        "            'num_classes': ImprovedNCSNConfig.num_classes,\n",
        "            'ngf': ImprovedNCSNConfig.ngf,\n",
        "            'use_attention': ImprovedNCSNConfig.use_attention,\n",
        "        }\n",
        "    }, final_path)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Improved NCSN training completed successfully!\")\n",
        "    print(f\"Final model saved to: {final_path}\")\n",
        "    print(f\"Improvements used:\")\n",
        "    print(f\" - Spectral Normalization\")\n",
        "    print(f\" - Self-Attention at 8x8 resolution\")\n",
        "    print(f\" - Exponential Moving Average (EMA)\")\n",
        "    print(f\" - Improved noise conditioning (output/sigma)\")\n",
        "    print(f\" - Better noise schedule (σ: {ImprovedNCSNConfig.sigma_begin} → {ImprovedNCSNConfig.sigma_end})\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    wandb.finish()\n",
        "\n",
        "    return model, sigmas\n",
        "\n",
        "\n",
        "# ==================== Run Training ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, sigmas = train_improved_ncsn()\n",
        "    print(\"Improved NCSN training completed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v7ncGW3n78y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
